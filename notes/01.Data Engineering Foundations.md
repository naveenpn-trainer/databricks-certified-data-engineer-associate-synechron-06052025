# Data Engineering Foundations

> Big Data refers to the data which is **large, fast** and complex type of **structured, semi-structed and unstructured**, generated from variety of different source, which b**ecomes difficult to store and process using a traditional processing system**

## Challenges of Big Data

The two main challenges w.r.t to Big Data

1. Storage : Distributed Storage System
2. Processing : MPP (Massive Parallel Processing Framework)

## Hadoop 2.x (Distributed Storage and Processing Framework)

> Apache Hadoop is a software framework that allows us to **store and process large datasets** **in parallel and distributed fashion**

Hadoop provides three main components

1. Storage Layer : HDFS(Hadoop Distributed File System)
2. Resource Management Layer : YARN (Yet Another Resource Negotiator)
3. Processing Layer : MapReduce

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeUNm-a0ODeH3koTgdPXFlsStB4RKuiCIkFRlb5nkD_FHE08Dv_54VekRVe_fM49BtJd309Oj-0DWBN4DcTlQhKMXTY0A3EKeyCiMkQzyNM9mW5xv8lYR0IvH-ZX8cIkthrR9ZkDkbTxgkNLrbSbOgdU2t4?key=Lcjgu0sLjm8U8i3A_14gRg)

### HDFS

## Apache Spark

> Apache Spark is an in-memory cluster computing framework designed to handle a wide range of big data workloads

**Application of Apache Spark**

1. Data Integration and ETL
2. Batch Computation
3. Stream Processing
4. Graph Computation
5. ML

**Important Points**

* Apache Spark is natively written using Scala language

### What is PySpark

> PySpark is the Python API for Apache Spark.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd9Emjzr6kaUTG0z6xQKkNYiHGx0EWzpz6vEzmfk8Gjjj1-lO4NoQUFAH3tuAJmLpnt_j1RtIII1HVLJ5DAjREjEhjAYZUI144Hmo2Mzh2Qy6WSy7YYrZv3QAxyhpf2gASiF51HBUs_pSbM7yPnKs9iWWNT?key=_he-T4Jq934AhrSZa-Be-g)



### Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfGcZSlp5zl22JVbGql7UZcQ2ZHgtCcc4DovfntlJUWkX8L8mQbaDDbHnjNVfzCFDzJAEPaB1Oj-c6lkQDlU9lEyI9HRFSgzRWNMnWjCR8sM8JCgEqyWNhLZt-Ku9MADJF7nx4u1_ACAl46muI9x8Ygzoka?key=_he-T4Jq934AhrSZa-Be-g)

### Spark Interactive Shell

1. Spark Shell

   ```
   spark-shell
   ```

   

2. PySpark Shell

   ```
   pyspark
   ```

   

### API's to Interact with Spark

1. Low Level API (RDD's) - Deprecated
2. High Level API - Spark SQL

### RDD's

> Resilient Distributed Dataset which are the building blocks of any spark application, a fundamental data structure in Apache Spark

### Partitions

> An rdd is collection of partitions.

> A Partition is a logical division of data stored in a node

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcjbuVxrQDtFg_Z0z9ozd9u1zH7QxYxgWAilm6-5Jb7TwX02lGISI6rwBaipSx9BG4OrbYxaR_6L-s_j_hG2WodVxSXwHIX8L9mli7c0-xQbY9EjuKY_HeelYE0sLaOkUuSWNVmhg5SbmTSyp1KVx_sDkL0?key=Dxp7lTxgvspH2ig-I7LuEw)

### Creating RDD's

* All methods to create RDD is present inside SparkContext (sc)

There are two main ways to create an RDD's

1. Create an RDD from Collection

   ```python
   L = list(range(1,101))
   rdd = sc.parallelize(L)
   type(rdd)
   ```

   

2. Create an RDD from External Source

   ```
   file_rdd = sc.textFile("c:/file01.txt")
   type(file_rdd)
   ```

   

### Operations

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf-i45tOSsOA_9yT1hjzB3OQ1q39RhG8vREh8mD0TABjTvv_Inu6KHg_o8GzEuexw7ekmlLJxD89W_lAe2UWVpICu_uel1G8NSlcRKTGkF3J8NalJBGgnhjwcCStfcvwTS5agmPcCGb21MUpw3TMg7tge_n?key=Dxp7lTxgvspH2ig-I7LuEw)









